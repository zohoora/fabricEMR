services:
  # PostgreSQL with pgvector extension for vector similarity search
  postgres:
    image: pgvector/pgvector:pg16
    restart: always
    environment:
      - POSTGRES_USER=medplum
      - POSTGRES_PASSWORD=medplum
    command:
      - 'postgres'
      - '-c'
      - 'listen_addresses=*'
      - '-c'
      - 'statement_timeout=60000'
      - '-c'
      - 'default_transaction_isolation=REPEATABLE READ'
      - '-c'
      - 'shared_preload_libraries=pg_stat_statements,auto_explain'
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U medplum']
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7
    restart: always
    command: redis-server --requirepass medplum
    ports:
      - '6379:6379'
    healthcheck:
      test: ['CMD', 'redis-cli', '-a', 'medplum', 'ping']
      interval: 10s
      timeout: 5s
      retries: 5

  # Medplum server container
  medplum-server:
    image: medplum/medplum-server:latest
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - '8103:8103'
    volumes:
      - ${MEDPLUM_CONFIG_PATH:-./medplum.config.json}:/usr/src/medplum/packages/server/medplum.config.json
    entrypoint: >
      sh -c "
      if [ -n '${MEDPLUM_CONFIG_PATH}' ]; then
        echo 'Config file found, running with custom config'
        node --require ./packages/server/dist/otel/instrumentation.js packages/server/dist/index.js file:/usr/src/medplum/packages/server/medplum.config.json
      else
        echo 'No config file found, running with default env settings'
        node --require ./packages/server/dist/otel/instrumentation.js packages/server/dist/index.js env
      fi
      "
    environment:
      MEDPLUM_PORT: 8103
      MEDPLUM_BASE_URL: 'http://localhost:8103/'
      MEDPLUM_APP_BASE_URL: 'http://localhost:3000/'
      MEDPLUM_STORAGE_BASE_URL: 'http://localhost:8103/storage/'

      MEDPLUM_DATABASE_HOST: 'postgres'
      MEDPLUM_DATABASE_PORT: 5432
      MEDPLUM_DATABASE_DBNAME: 'medplum'
      MEDPLUM_DATABASE_USERNAME: 'medplum'
      MEDPLUM_DATABASE_PASSWORD: 'medplum'

      MEDPLUM_REDIS_HOST: 'redis'
      MEDPLUM_REDIS_PORT: 6379
      MEDPLUM_REDIS_PASSWORD: 'medplum'

      MEDPLUM_BINARY_STORAGE: 'file:./binary/'
      MEDPLUM_SUPPORT_EMAIL: '"Medplum" <support@medplum.com>'
      MEDPLUM_GOOGLE_CLIENT_ID: '397236612778-c0b5tnjv98frbo1tfuuha5vkme3cmq4s.apps.googleusercontent.com'
      MEDPLUM_GOOGLE_CLIENT_SECRET: ''
      MEDPLUM_RECAPTCHA_SITE_KEY: '6LfHdsYdAAAAAC0uLnnRrDrhcXnziiUwKd8VtLNq'
      MEDPLUM_RECAPTCHA_SECRET_KEY: '6LfHdsYdAAAAAH9dN154jbJ3zpQife3xaiTvPChL'
      MEDPLUM_MAX_JSON_SIZE: '1mb'
      MEDPLUM_MAX_BATCH_SIZE: '50mb'
      MEDPLUM_BOT_LAMBDA_ROLE_ARN: ''
      MEDPLUM_BOT_LAMBDA_LAYER_NAME: 'medplum-bot-layer'
      MEDPLUM_VM_CONTEXT_BOTS_ENABLED: 'true'
      MEDPLUM_DEFAULT_BOT_RUNTIME_VERSION: 'vmcontext'
      MEDPLUM_ALLOWED_ORIGINS: '*'
      MEDPLUM_INTROSPECTION_ENABLED: 'true'
      MEDPLUM_SHUTDOWN_TIMEOUT_MILLISECONDS: 30000

    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'fetch("http://localhost:8103/healthcheck").then(r => r.json()).then(console.log).catch(() => { process.exit(1); })',
        ]
      interval: 30s
      timeout: 10s
      retries: 5

  # Medplum app container (web UI)
  medplum-app:
    image: medplum/medplum-app:latest
    restart: always
    depends_on:
      medplum-server:
        condition: service_healthy
    ports:
      - '3000:3000'
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3000']
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # AI SERVICES
  # ============================================

  # Ollama - Local LLM inference (PHI-safe)
  ollama:
    image: ollama/ollama:latest
    restart: always
    ports:
      - '11434:11434'
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ['CMD-SHELL', 'ollama list || exit 0']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # LiteLLM Gateway - AI Control Plane
  # Routes LLM requests, enforces policies, logs everything
  llm-gateway:
    image: ghcr.io/berriai/litellm:main-latest
    restart: always
    depends_on:
      ollama:
        condition: service_healthy
      postgres:
        condition: service_healthy
    ports:
      - '8080:4000'
    environment:
      - LITELLM_MASTER_KEY=sk-medplum-ai
      - DATABASE_URL=postgresql://medplum:medplum@postgres:5432/litellm
      - OLLAMA_API_BASE=http://ollama:11434
      # Uncomment and set these for cloud LLM access:
      # - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      # - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    volumes:
      - ./config/litellm-config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:4000/health || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # NOTE: For embeddings, use Ollama with nomic-embed-text model
  # Run: docker exec medplum-ollama-1 ollama pull nomic-embed-text
  # This provides 768-dimensional embeddings compatible with our schema

volumes:
  postgres_data:
  ollama_data:
